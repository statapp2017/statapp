# Packages -----------------------------
#install.packages("twitteR", dependencies = TRUE)
#install.packages("base64enc")
#install.packages("openssl")
#install.packages("koRpus")
#install.packages("taskscheduleR")

rm(list <- ls())
library(taskscheduleR)
library(base64enc)
library(koRpus)
library(openssl)
library(stringr)
library(twitteR)
library(devtools)
library(httr)

# Connexion au serveur Twitter -----------------------------
consumer_key <- "omimBnQiiwuOgqUPYxC501p0O"
consumer_secret <-"AuFzlqP7OKXD2xKOG6jnf6hYLKWcIiNsIlsRHDNS1ochNJuCtt"
access_token <- "826780938382241793-jOjPTTDTiLg162tvswNueIue1WM7gDc"
access_secret <- "Znbbt4Tgy5QFjdpHlI7ZXDWzPZArf96ZoZi0pKz8bfkFg"
setup_twitter_oauth(consumer_key, consumer_secret, access_token, access_secret)

# Recuperation de Tweets -----------------------------
# Fonction de recuperation des Tweets et reponses si possible pour un identifiant de type @Mon_AXA
recuperer <- function(identifier, insurance) {
  Uid <- getUser(identifier)$id
  tweets_df <- twListToDF(userTimeline(iden, n = 1000))
  tweets_df <- tweets_df[!is.na(tweets_df$replyToSID), ]
  tweets_df$conv <- tweets_df$replyToSN
  for (i in 1:nrow(tweets_df)) {
    SID <- tweets_df$replyToSID[i]
    options(show.error.messages = FALSE)
    try(identifiant <- getUser(tweets_df$replyToUID[i]))
    try(t <- twListToDF(userTimeline(identifiant, n = 100)))
    try(t$conv <- c(tweets_df$replyToSN[i]))
    try(t <- t[t$id == SID, ])
    try(tweets_df <- rbind(tweets_df, t))
  }
  tweets_df$assurance <- rep(insurance, nrow(tweets_df))
  tweets_df[order(df$conv), ]
}
p <- recuperation("@Mon_AXA")
liste_assu <- read.csv2("C:/Users/AC-te/Downloads/workspace/liste_assu.csv")

# Extraction pour toutes les assurances reperees
extraction <- function(liste_assu, ancien_fichier) {
  ancien <- read.csv2(ancien_fichier)
  for (i in 1:nrow(liste_assu)) {
    ancien <- rbind(ancien, recuperation(liste_assu$iden[i], liste_assu$assurance[i]))
  }
  write.csv(ancien, ancien_fichier)
}

taskscheduler_create(taskname = "automate", rscript = "D:/ENSAE/2èmeannée/Statsapp/Statsapp.R", schedule = "ONCE")
taskcheduler_runnow("automate")

# Lemmatisation dataframe -----------------------------
# Lemmatisation d'un texte contenue dans le fichier situe dans le dossier a l'adresse file
lemmatizer <- function(file) {
  # a la ligne suivante dans path = "..." mettre l'adresse du dossier TreeTagger
  text <- data.frame(treetag(file, treetagger = "manual", TT.tknz = FALSE, lang = "fr",encoding = "Latin1",TT.options = list(path = "C:/TreeTagger", preset = "fr"))@TT.res[, c(1, 2, 3)])
  chain <- ""
  for (i in 1:nrow(text)) {
    if (text$lemma[i] == "<unknown>") {
      chain <- paste(chain, text$token[i])
    }
    else {
      chain <- paste(chain, text$lemma[i])
    }
  }
  chain
}

# Lemmatisation d'un dataframe du type de celui issu de l'extraction de Tweet
lemma_dataframe <- function(data) {
  for (i in 1:nrow(data)) {
    file.create(paste(data$id[i], ".txt", sep = ""))
    write.table(data$text[i], file = paste(data$id[i], ".txt", sep = ""))
    data$lemme[i] <- str_sub(lemmatizer(paste(data$id[i], ".txt", sep = "")), 19)
    file.remove(paste(data$id[i], ".txt", sep = ""))
  }
  data
}

# Debut de la labelisation a la main -----------------------------
t <- read.table("D:/ENSAE/Sentiment.txt")
trad <- read.csv("D:/ENSAE/traduc.csv")[, c(3:5)]
FEEL <- read.csv2("D:/ENSAE/2 ème année/Stats app/FEEL.csv", encoding="UTF-8")[, c(2, 3)]
sentiment <- function(phrase) {
  FEEL <- read.csv2("D:/ENSAE/2 ème année/Stats app/FEEL.csv", encoding="UTF-8")[, c(2, 3)]
  tab_negation <- c("n", "ne", "ni", "pas", "jamais", "aucun", "guère", "nullement", "aucunement", "rien")
  negation <- data.frame(tab_negation)
  # Lemmatisation
  texte <- lemmatizer(phrase)
}
