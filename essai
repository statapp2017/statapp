#install.packages("twitteR", dependencies = TRUE)
#install.packages("base64enc")
#install.packages("openssl")
#install.packages("koRpus")
#install.packages("taskscheduleR")

rm(list=ls())
library(taskscheduleR)
library(base64enc)
library(koRpus)
library(openssl)
library(stringr)
library(twitteR)
library(devtools)
library(httr)

#Connexion au serveur twitter
consumer_key <-"omimBnQiiwuOgqUPYxC501p0O"
consumer_secret<-"AuFzlqP7OKXD2xKOG6jnf6hYLKWcIiNsIlsRHDNS1ochNJuCtt"
access_token <-"826780938382241793-jOjPTTDTiLg162tvswNueIue1WM7gDc"
access_secret<-"Znbbt4Tgy5QFjdpHlI7ZXDWzPZArf96ZoZi0pKz8bfkFg"
setup_twitter_oauth(consumer_key,consumer_secret,access_token,access_secret)

#Recuperation tweets-----------------------------------------------------------------------------------------

#Fonction de recuperation des tweets et réponses si possible pour un identifiant de type @Mon_AXA
recuperer<-function(identifier,insurance){
  Uid <- getUser(identifier)$id
  tweets_df <- twListToDF(userTimeline(iden, n = 1000))
  tweets_df <- tweets_df[!is.na(tweets_df$replyToSID),]
  tweets_df$conv <- tweets_df$replyToSN
  for (i in 1:nrow(tweets_df)){
    SID <- tweets_df$replyToSID[i]
    options(show.error.messages = FALSE)
    try(identifiant <- getUser(tweets_df$replyToUID[i]))
    try(t <- twListToDF(userTimeline(identifiant, n = 100)))
    try(t$conv <- c(tweets_df$replyToSN[i]))
    try(t <- t[t$id==SID,])
    try(tweets_df <- rbind(tweets_df,t))
  }
  tweets_df$assurance <- rep(insurance,nrow(tweets_df))
  tweets_df[order(df$conv),]
}
p <- recuperation("@Mon_AXA")
liste_assu <- read.csv2("C:/Users/AC-te/Downloads/workspace/liste_assu.csv ")

#Extraction pour toutes les assurances repérées
extraction <- function(liste_assu,ancien_fichier){
  ancien <- read.csv2(ancien_fichier)
  for (i in 1:nrow(liste_assu)){
    ancien <- rbind(ancien,recuperation(liste_assu$iden[i],liste_assu$assurance[i]))
  }
  write.csv(ancien,ancien_fichier)
}

taskscheduler_create(taskname="automate",rscript="D:/ENSAE/2èmeannée/Statsapp/Statsapp.R",schedule = "ONCE")
taskcheduler_runnow("automate")

#Lemmatisation dataframe-------------------------------------------------------------------------------------

#lemmatization d'un texte contenue dans le fichier situé dans le dossier à l'adresse file
lemmatizer<-function(file){
  #a la ligne suivante dans path=" ..." mettre l'adresse du dossier TreeTagger
  text <- data.frame(treetag(file,treetagger = "manual",TT.tknz=FALSE,lang="fr",encoding="Latin1",TT.options=list(path="C:/TreeTagger",preset = "fr"))@TT.res[,c(1,2,3)])
  chaine=""
  for (i in 1:nrow(text)){
    if (text$lemma[i]=="<unknown>"){
      chaine <- paste(chaine,text$token[i])
    }
    else {
      chaine <- paste(chaine,text$lemma[i])
    }
  }
  chaine
}

#lemmatization d'un dataframe du type de celui issu de l'extraction de tweet
lemma_dataframe <- function(data){
  for (i in 1:nrow(data)){
    file.create(paste(data$id[i],".txt",sep=""))
    write.table(data$text[i],file=paste(data$id[i],".txt",sep=""))
    data$lemme[i]<-str_sub(lemmatizer(paste(data$id[i],".txt",sep="")),19)
    file.remove(paste(data$id[i],".txt",sep=""))
  }
  data
}

#Debut de la labélisation à la main--------------------------------------------------------------------------

t <- read.table("D:/ENSAE/Sentiment.txt")
trad <- read.csv("D:/ENSAE/traduc.csv")[ , c(3:5)]
FEEL <- read.csv2("D:/ENSAE/2 ème année/Stats app/FEEL.csv",encoding="UTF-8")[,c(2,3)]
sentiment <- function(phrase){
  FEEL<-read.csv2("D:/ENSAE/2 ème année/Stats app/FEEL.csv",encoding="UTF-8")[,c(2,3)]
  negation<-data.frame(Negation=c("n","ne","ni","pas","jamais","aucun","guère","nullement","aucunement","rien"))
  #lemmatization
  texte <- lemmatizer(phrase)
}
